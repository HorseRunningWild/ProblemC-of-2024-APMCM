{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author：WILD HORSE**\n",
    "**数学建模小组成员：**  \n",
    "队长 贾梓杏 SYSU-物理与天文学院2023级   \n",
    "队员 夏解金鑫 SYSU-物理与天文学院2023级  \n",
    "队员 刘张弛 SYSU-数学学院（珠海）2023级  \n",
    "建模思路在公众号已经分享，此部分为代码展示（不全，但接近全）  \n",
    "关于data，想来Q1 X和Q1 Y等等含义比较显然的名称就不再解释；此处提供给大家的数据中，我们的结果被我们移除，而是给了大家处理好之后的原始数据  \n",
    "例如将欧盟的主要国家的CPI加权求和、对年度数据进行插值等等  \n",
    "如果对Data been Used的部分感兴趣，可以去猜一下文件里面装的是什么--我其实挺建议大家这麽做的，因为拿到手上最初的数据也就那些，甚至还更难看  \n",
    "希望本文对大家有帮助！感谢大家的支持！也欢迎大家分享给其他小伙伴！  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**启用LaTeX渲染**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 启用LaTeX文本渲染\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One：年度数据插值与PLS回归  \n",
    "***对大指标进行插值***  \n",
    "此处的插值逻辑是通用的，改文件地址和列名即可，注意要保留一个year列，是代码索引的关键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline, interp1d, Akima1DInterpolator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = \"YOUR FILE PATH\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# 将年份转换为标准格式\n",
    "df.columns = ['year', 'Cat number', 'Dog number', 'Cat total consumption', 'Dog total consumption', \n",
    "              'Cat dog food ratio', 'Cat dog food consumption', 'Medical consumption ratio','Other pets amount','GDP per person','Gender ration']\n",
    "\n",
    "# 检查读取的数据\n",
    "print(\"原始数据:\")\n",
    "print(df)\n",
    "\n",
    "# 进行三次样条插值、线性插值和Akima插值\n",
    "years = df['year'].values\n",
    "start_year = years.min()\n",
    "end_year = years.max()\n",
    "\n",
    "# 确保years是递增的\n",
    "if not np.all(np.diff(years) > 0):\n",
    "    raise ValueError(\"`years` must be strictly increasing sequence.\")\n",
    "\n",
    "# 计算月份数量\n",
    "num_months = int((end_year - start_year ) * 12)\n",
    "\n",
    "# 生成从开始年份到结束年份的月度数据\n",
    "months = np.linspace(start_year, end_year, num_months)\n",
    "\n",
    "# 函数进行插值，并确保输出数据范围合理\n",
    "def interpolate_cubic(column):\n",
    "    values = df[column].values\n",
    "    spline = CubicSpline(years, values)\n",
    "    interpolated_values = spline(months)\n",
    "    return interpolated_values\n",
    "\n",
    "def interpolate_linear(column):\n",
    "    values = df[column].values\n",
    "    linear_interp = interp1d(years, values, kind='linear')\n",
    "    interpolated_values = linear_interp(months)\n",
    "    return interpolated_values\n",
    "\n",
    "def interpolate_akima(column):\n",
    "    values = df[column].values\n",
    "    akima_interp = Akima1DInterpolator(years, values)\n",
    "    interpolated_values = akima_interp(months)\n",
    "    return interpolated_values\n",
    "\n",
    "# 对每一列数据进行插值，并确保插值结果不会出现过大的数值\n",
    "columns_to_interpolate = ['Cat number', 'Dog number', 'Cat total consumption', 'Dog total consumption', \n",
    "                          'Cat dog food ratio', 'Cat dog food consumption', 'Medical consumption ratio','Other pets amount','GDP per person','Gender ration']\n",
    "interpolated_data_cubic = {col: interpolate_cubic(col) for col in columns_to_interpolate}\n",
    "interpolated_data_linear = {col: interpolate_linear(col) for col in columns_to_interpolate}\n",
    "interpolated_data_akima = {col: interpolate_akima(col) for col in columns_to_interpolate}\n",
    "\n",
    "\n",
    "# 生成包含月度数据的DataFrame（这里选择Akima插值的结果）\n",
    "\n",
    "dates = pd.date_range(start=f\"{int(start_year)}-01-01\", periods=num_months, freq='MS')\n",
    "dom_big_IP = pd.DataFrame({\n",
    "    'month': dates\n",
    "})\n",
    "dom_big_IP = dom_big_IP.assign(**{col: interpolated_data_akima[col] for col in columns_to_interpolate})\n",
    "\n",
    "# 保存结果到Excel文件\n",
    "output_path = \"D:\\\\FSS\\\\MMM\\\\APMCM\\\\2024FORcon\\\\dom_big_IP.xlsx\"\n",
    "dom_big_IP.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"月度数据已生成并保存到:\", output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*注，上述代码在生成的文件中会多出一列‘month’，使用时删除即可*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**套路化相关性分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path ='YOUR FILE PATH'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 数据清洗和初步检查\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nData Types:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# 处理缺失值（如果存在）\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 计算相关性矩阵\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# 设置绘图风格\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# 创建画布和轴\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': r'$\\bf{Correlation}$'}, square=True, linewidths=.5, annot_kws={\"size\": 10})\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title(r'$\\bf{Monthly \\ Index \\ Correlation \\ Matrix}$', fontsize=16)\n",
    "plt.xlabel(r'$\\bf{Variables}$', fontsize=14)\n",
    "plt.ylabel(r'$\\bf{Variables}$', fontsize=14)\n",
    "\n",
    "# 显示图形\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 偏最小二乘回归\n",
    "**数据读取与标准化**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 读取数据\n",
    "file_path_y = r\"D:\\FSS\\MMM\\APMCM\\2024FORcon\\dom_big_IP.xlsx\"\n",
    "file_path_x = r\"D:\\FSS\\MMM\\APMCM\\2024FORcon\\domestic small index.xlsx\"\n",
    "# YOUR FILE PATH\n",
    "\n",
    "df_y = pd.read_excel(file_path_y)\n",
    "df_x = pd.read_excel(file_path_x)\n",
    "\n",
    "# 提取Y和X的数据\n",
    "y = df_y.iloc[:, 0].values.reshape(-1, 1)  # 第一个Y指标\n",
    "x = df_x.iloc[:, :3].values               # 前三个X指标\n",
    "\n",
    "# 标准化Y和X\n",
    "scaler_y = StandardScaler()\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "x_scaled = scaler_x.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最佳主成分计算**  \n",
    "*这里可以注意一下，最后的可视化效果应该是四五个大指标的Cvscores绘制在同一张图上*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用交叉验证确定最佳主成分数\n",
    "n_components_range = range(1, 4)\n",
    "cv_scores = []\n",
    "\n",
    "for n in n_components_range:\n",
    "    pls = PLSRegression(n_components=n)\n",
    "    scores = cross_val_score(pls, x_scaled, y_scaled, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "best_n_components = n_components_range[np.argmax(cv_scores)]\n",
    "print(f\"Best number of components: {best_n_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***回归方程计算***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最佳主成分数进行PLS回归\n",
    "pls_final = PLSRegression(n_components=best_n_components)\n",
    "pls_final.fit(x_scaled, y_scaled.ravel())\n",
    "\n",
    "# 计算相关系数\n",
    "coefficients = pls_final.coef_\n",
    "print(\"Coefficients for each X variable:\")\n",
    "print(coefficients.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two：小指标LsTm回归  \n",
    "*数据准备*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# 加载数据\n",
    "file_path = r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\domestic small index.xlsx'\n",
    "# YOUR FILE PATH\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 归一化数据\n",
    "scalers = {}\n",
    "scaled_data = {}\n",
    "\n",
    "for column in data.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_column = scaler.fit_transform(data[[column]])\n",
    "    scalers[column] = scaler\n",
    "    scaled_data[column] = scaled_column\n",
    "\n",
    "# 准备数据集\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***模型训练***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 12  # 使用前8个月的数据来预测下一个月的数据\n",
    "Xs, ys = {}, {}\n",
    "\n",
    "for column in data.columns:\n",
    "    X, y = create_dataset(scaled_data[column], time_step=time_step)\n",
    "    Xs[column] = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    ys[column] = y\n",
    "\n",
    "# 构建并训练LSTM模型\n",
    "models = {}\n",
    "\n",
    "for column in data.columns:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=20, return_sequences=True, input_shape=(Xs[column].shape[1], 1)))\n",
    "    model.add(LSTM(units=20, return_sequences=False))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(Xs[column], ys[column], epochs=100, batch_size=1, verbose=1)\n",
    "    models[column] = model\n",
    "\n",
    "# 评估模型效果\n",
    "train_predicts, test_predicts = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***评估模型效果***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型效果\n",
    "train_predicts, test_predicts = {}, {}\n",
    "\n",
    "for column in data.columns:\n",
    "    train_predict = models[column].predict(Xs[column])\n",
    "    train_predict = scalers[column].inverse_transform(train_predict)\n",
    "    train_predicts[column] = train_predict\n",
    "    \n",
    "    # 计算训练集上的误差指标\n",
    "    mse_train = mean_squared_error(ys[column], train_predict)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(scalers[column].inverse_transform(ys[column].reshape(-1, 1)), train_predict)\n",
    "    \n",
    "    print(f\"{column} - Train MSE: {mse_train}, RMSE: {rmse_train}, MAE: {mae_train}\")\n",
    "    \n",
    "    # 创建测试数据集\n",
    "    x_input = scaled_data[column][-time_step:].reshape(1, -1)\n",
    "    temp_input_list = list(x_input.flatten())\n",
    "    \n",
    "    lst_output = []\n",
    "    n_steps = time_step\n",
    "    next_n_days = 36\n",
    "    \n",
    "    i = 0\n",
    "    while(i < next_n_days):\n",
    "        \n",
    "        if len(temp_input_list) > n_steps:\n",
    "            x_input = np.array(temp_input_list[1:])\n",
    "            x_input = x_input.reshape(1, -1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            \n",
    "            yhat = models[column].predict(x_input, verbose=0)\n",
    "            temp_input_list.extend(yhat[0].tolist())\n",
    "            temp_input_list = temp_input_list[1:]\n",
    "            \n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i += 1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            yhat = models[column].predict(x_input, verbose=0)\n",
    "            temp_input_list.extend(yhat[0].tolist())\n",
    "            \n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i += 1\n",
    "            \n",
    "    test_predict = np.array(lst_output).reshape(-1, 1)\n",
    "    test_predict = scalers[column].inverse_transform(test_predict)\n",
    "    test_predicts[column] = test_predict.flatten()  # 展平数组 不能用LSTM的格式 去储存\n",
    "\n",
    "# 将预测结果保存到新的Excel文件中\n",
    "predictions_df = pd.DataFrame(test_predicts)\n",
    "# YOUR FILE PATH\n",
    "predictions_df.to_excel(r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\dsi_predic.xlsx', index=False)\n",
    "\n",
    "print(\"预测结果已保存到 D:\\\\FSS\\\\MMM\\\\APMCM\\\\2024FORcon\\\\dsi_predic.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***结果可视化***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for column in data.columns:\n",
    "    plt.plot(range(time_step, len(train_predicts[column]) + time_step), train_predicts[column], label=f'{column} Train Predict')\n",
    "    plt.plot(range(len(train_predicts[column]) + time_step, len(train_predicts[column]) + time_step + len(test_predicts[column])), \n",
    "             test_predicts[column], label=f'{column} Future Predict')\n",
    "    plt.scatter(range(time_step, len(train_predicts[column]) + time_step), scalers[column].inverse_transform(ys[column].reshape(-1, 1)),\n",
    "                color='red', s=5, label=f'{column} Actual Values' if column == data.columns[0] else \"\")\n",
    "\n",
    "plt.title('Train and Future Prediction Comparison')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***补充：关于超参数调优***  \n",
    "超参数调用的思路较多，此处推荐选择不超过五个超参数，使用GA或者网格搜索，如果你有很大算力另当别论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part Three:** 使用X的预测值最终计算Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取未来的X数据\n",
    "file_path_future_x = r\"D:\\FSS\\MMM\\APMCM\\2024FORcon\\dsi_predic.xlsx\"\n",
    "# YOUR FILE PATH\n",
    "df_future_x = pd.read_excel(file_path_future_x)\n",
    "\n",
    "# 提取未来的X数据\n",
    "future_x = df_future_x.iloc[:, :3].values\n",
    "\n",
    "# 对未来的X数据进行标准化处理\n",
    "future_x_scaled = scaler_x.transform(future_x)\n",
    "\n",
    "# 使用训练好的PLS模型进行预测\n",
    "predicted_y_scaled = pls_final.predict(future_x_scaled)\n",
    "\n",
    "# 将预测结果反标准化回到原始尺度\n",
    "predicted_y = scaler_y.inverse_transform(predicted_y_scaled)\n",
    "\n",
    "# 创建一个DataFrame来存储预测结果\n",
    "df_predictions = pd.DataFrame(predicted_y, columns=['Predicted_Y'])\n",
    "\n",
    "# 打印预测结果\n",
    "print(\"Predicted Y values (original scale):\")\n",
    "print(df_predictions)\n",
    "\n",
    "# 可选：将预测结果保存到Excel文件\n",
    "output_file_path = r\"D:\\FSS\\MMM\\APMCM\\2024FORcon\\predictions.xlsx\"\n",
    "df_predictions.to_excel(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One：如Q1的PLS  \n",
    "此处首先补充说明一下欧盟的数据来源。因为在很多数据库里，并不存在“欧盟”的许多数据，而是各个欧洲国家自己的数据  \n",
    "于是为了不使代表性，我们选取了八个欧洲国家作为欧盟的代表，具体见表格，选取依据是宠物猫狗持有量的占比--FEDIAF报告  \n",
    "然后根据持有量占比，将各国的指标加权加和。下列代码为取均值。  \n",
    "并且此处的数据也不建议再动，因为可能改了格式，再去Debug较为麻烦\n",
    "***欧盟CPI数据处理***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\EU CPI.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 提取出第F列\n",
    "cpi_data = data['obs_value']\n",
    "\n",
    "# 倒序排列数据\n",
    "cpi_data_reversed = cpi_data[::-1]\n",
    "\n",
    "# 计算每个月份的平均CPI\n",
    "num_countries = 8\n",
    "monthly_avg_cpi = []\n",
    "\n",
    "for i in range(60):\n",
    "    monthly_cpi_sum = sum(cpi_data_reversed[i::60])\n",
    "    monthly_avg_cpi.append(monthly_cpi_sum / num_countries)\n",
    "\n",
    "# 将结果输出到新的CSV文件 'EU CPI av.csv'\n",
    "output_file_path = r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\EU CPI av.csv'\n",
    "pd.DataFrame({'Monthly Avg CPI': monthly_avg_cpi}).to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***欧盟UE数据处理***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = r\"D:\\FSS\\MMM\\APMCM\\2024FORcon\\EU unemployment.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 提取出第F列（假设列名为'F'）\n",
    "cpi_data = data['obs_value']\n",
    "\n",
    "# 倒序排列数据\n",
    "cpi_data_reversed = cpi_data[::-1]\n",
    "\n",
    "# 计算每个月份的平均CPI\n",
    "num_countries = 8\n",
    "monthly_avg_cpi = []\n",
    "\n",
    "for i in range(60):\n",
    "    monthly_cpi_sum = sum(cpi_data_reversed[i::60])\n",
    "    monthly_avg_cpi.append(monthly_cpi_sum / num_countries)\n",
    "\n",
    "# 将结果输出到新的CSV文件 'EU CPI av.csv'\n",
    "output_file_path = r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\EU unemployment av.csv'\n",
    "pd.DataFrame({'Monthly Avg CPI': monthly_avg_cpi}).to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two:结合宠物食品成本，先对宠物数量插值再乘以成本\n",
    "**读取数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "data = {\n",
    "    'EU': {' Cat': [7900, 8131.1, 8362.2, 7792.8, 7908,7530], 'Dog': [7900.8, 7585.8, 7270.8, 6686.6, 6862,6550]},\n",
    "    'Russia': {' Cat': [2280, 2275, 2295, 2315, 2326,2250], 'Dog': [1680, 1710, 1755, 1755.8, 1764,1750]},\n",
    "    'Japan': {' Cat': [977.8, 960, 894.6, 887.3, 906.9,965], 'Dog': [879.7, 850, 710.6, 705.3, 684.4,890]},\n",
    "    'Brazil': {' Cat': [2710, 2700, 2710, 3360, 3420,2710], 'Dog': [5810, 5800, 5840, 6780, 6840,5810]},\n",
    "    'America': {' Cat': [9420, 6500, 9420, 7380, 7380,8970], 'Dog': [8970, 8500, 8970, 8970, 8010,9420]}\n",
    "}\n",
    "# 提取年份\n",
    "# years = list(range(2019, 2024))\n",
    "# months = np.linspace(0, 59, 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***三次样条插值***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 每个国家每年末的数据点索引\n",
    "years = np.arange(2018, 2024)\n",
    "\n",
    "# 新的时间点（每个月）\n",
    "months = np.linspace(2018, 2023, num=60)\n",
    "\n",
    "# interpolated_data = {}\n",
    "for country, animals in data.items():\n",
    "    interpolated_animals = {}\n",
    "    for animal, values in animals.items():\n",
    "        # 确保数据长度一致\n",
    "        if len(values) != len(years):\n",
    "            continue\n",
    "        \n",
    "        # 创建三次样条插值函数\n",
    "        cs = CubicSpline(years, values)\n",
    "        \n",
    "        # 计算新的月份数据点\n",
    "        monthly_values = cs(months)\n",
    "        \n",
    "        # # 删除第一个数据点（2018年末的数据）\n",
    "        # monthly_values = monthly_values[1:]\n",
    "        \n",
    "        # 生成键\n",
    "        key = f\"{country[0].lower()}_{animal}_pm\"\n",
    "        interpolated_animals[key] = monthly_values\n",
    "    \n",
    "    interpolated_data[country.lower()] = interpolated_animals\n",
    "\n",
    "# 打印结果示例\n",
    "for country, animals in interpolated_data.items():\n",
    "    print(f\"{country}:\")\n",
    "    for animal, values in animals.items():\n",
    "        print(f\"  {animal}: {values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 储存插值后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = list(data.keys())\n",
    "pets = set([pet for pet_data in data.values() for pet in pet_data])\n",
    "print(interpolated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字典转换为DataFrame\n",
    "data_frames = {}\n",
    "for region, data in interpolated_data.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    data_frames[region] = df\n",
    "\n",
    "# 写入Excel文件\n",
    "with pd.ExcelWriter('interpolated_data.xlsx', engine='openpyxl') as writer:\n",
    "    for region, df in data_frames.items():\n",
    "        df.to_excel(writer, sheet_name=region, index=False)\n",
    "\n",
    "print(\"数据已成功写入 Excel 文件\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制图表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***插值后的可视化***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置高级科研配色\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\interpolated_data.xlsx'\n",
    "regions = ['America','Brazil']\n",
    "\n",
    "# 创建一个新的图形\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 定义月份列表\n",
    "months = pd.date_range(start='2019-01', periods=60, freq='M')\n",
    "\n",
    "\n",
    "# 绘制每个地区的数据\n",
    "for i, region in enumerate(regions):\n",
    "    # 读取每个地区的数据\n",
    "    df = pd.read_excel(file_path, sheet_name=region)\n",
    "    \n",
    "    # 添加日期列\n",
    "    df['Date'] = months\n",
    "    \n",
    "    # 获取颜色\n",
    "    cat_color = sns.color_palette()[i * 2]\n",
    "    dog_color = sns.color_palette()[i * 2 + 1]\n",
    "    \n",
    "    # 定义线条样式\n",
    "    cat_linestyle = '-'   # 实线\n",
    "    dog_linestyle = '-.'  # 虚线\n",
    "    \n",
    "    # 绘制猫的数据\n",
    "    line_cat, = plt.plot(df['Date'], df.iloc[:, 0], label=f'{region} Cats', color=cat_color, linestyle=cat_linestyle, linewidth=2)\n",
    "    \n",
    "    # 绘制狗的数据\n",
    "    line_dog, = plt.plot(df['Date'], df.iloc[:, 1], label=f'{region} Dogs', color=dog_color, linestyle=dog_linestyle, linewidth=2.5)\n",
    "    \n",
    "    # 在同一个地区的猫和狗数量曲线之间填充颜色区域\n",
    "    plt.fill_between(df['Date'], df.iloc[:, 0], df.iloc[:, 1], color=cat_color, alpha=0.2)\n",
    "\n",
    "# 添加网格\n",
    "plt.grid(True, which='both', linestyle='-.', linewidth=0.5)\n",
    "\n",
    "\n",
    "# 设置图形属性\n",
    "plt.title(r'\\textbf{Pet Population Trends }', fontsize=16)\n",
    "plt.xlabel(r'\\textbf{Year}', fontsize=14)\n",
    "plt.ylabel(r'\\textbf{Number of Pets}', fontsize=14)\n",
    "plt.legend(fontsize=12, title=r'\\textbf{Legend}')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One ：类似第一问，首先是PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two：还是类似第一问，接着是LSTM的自变量预测  \n",
    "**为什么这两个Part没有代码呢？因为当时跑代码的时候是直接将第一问的文件地址给改了**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two：Reg to Export--MLP  \n",
    "**这里的数据请大家自己跑后进行验证，此处仅为试验。**  \n",
    "正确的做法是将Global Demand和中国宠物产业生产总值一并作为X使用MLP回归到作为Y的Export上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 读取CSV文件\n",
    "china_exports = pd.read_csv(r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\China Total Export.csv')\n",
    "global_demand = pd.read_csv(r'D:\\FSS\\MMM\\APMCM\\2024FORcon\\Global Total Demand.csv')\n",
    "\n",
    "\n",
    "# 假设两列数据已经对齐并且有相同的索引\n",
    "X = global_demand.values.reshape(-1, 1)\n",
    "y = china_exports.values.reshape(-1, 1)\n",
    "\n",
    "# 数据标准化\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# 构建MLP模型\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=1, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# 训练和评估模型\n",
    "model = build_model()\n",
    "history = model.fit(X_scaled, y_scaled, epochs=100, batch_size=8, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 使用交叉验证评估模型\n",
    "cv_scores = cross_val_score(model, X_scaled, y_scaled, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse_scores = np.sqrt(-cv_scores)\n",
    "\n",
    "print(f'Cross-validated RMSE scores: {cv_rmse_scores}')\n",
    "print(f'Mean CV RMSE: {np.mean(cv_rmse_scores)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'D:/FSS/MMM/APMCM/2024FORcon/Export_to_A_petfood.xlsx'\n",
    "# YOUR FILE PATH\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 将Date列转换为日期格式\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "\n",
    "# 设置Date为索引，并设置频率\n",
    "data.set_index('Date', inplace=True)\n",
    "data = data.asfreq('MS')\n",
    "\n",
    "# 定义关税变化日期\n",
    "tariff_change_dates = [pd.Timestamp('2018-10-01'), pd.Timestamp('2019-05-01')]\n",
    "\n",
    "# 定义时间窗口（五个月）\n",
    "window_size = 7\n",
    "\n",
    "# 定义一个函数进行事件研究分析\n",
    "def event_study_analysis(data, event_date, window_size):\n",
    "    # 定义估计窗口和事件窗口\n",
    "    estimation_window_end = event_date - timedelta(days=1)\n",
    "    estimation_window_start = estimation_window_end - pd.DateOffset(months=window_size)\n",
    "    event_window_start = event_date\n",
    "    event_window_end = event_window_start + pd.DateOffset(months=window_size)\n",
    "    \n",
    "    estimation_window = data.loc[estimation_window_start:estimation_window_end]\n",
    "    event_window = data.loc[event_window_start:event_window_end]\n",
    "    \n",
    "    # 建立基准模型（使用ARIMA模型）\n",
    "    model = ARIMA(estimation_window['KG'], order=(3, 1, 3))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # 预测事件窗口内的正常出口量\n",
    "    forecast = model_fit.get_forecast(steps=len(event_window))\n",
    "    normal_export_volume = forecast.predicted_mean\n",
    "    \n",
    "    # 计算异常出口量\n",
    "    actual_export_volume = event_window['KG']\n",
    "    abnormal_export_volume = actual_export_volume - normal_export_volume\n",
    "    \n",
    "    # 计算平均异常出口量（AAR）和累积异常出口量（CAR）\n",
    "    AAR = abnormal_export_volume.mean()\n",
    "    CAR = abnormal_export_volume.cumsum()\n",
    "    \n",
    "    return normal_export_volume, abnormal_export_volume, AAR, CAR, actual_export_volume\n",
    "\n",
    "# 分析每个关税变化事件\n",
    "results = {}\n",
    "for event_date in tariff_change_dates:\n",
    "    normal_export_volume, abnormal_export_volume, AAR, CAR, actual_export_volume = event_study_analysis(data, event_date, window_size)\n",
    "    \n",
    "    results[event_date] = {\n",
    "        'normal_export_volume': normal_export_volume,\n",
    "        'abnormal_export_volume': abnormal_export_volume,\n",
    "        'AAR': AAR,\n",
    "        'CAR': CAR,\n",
    "        'actual_export_volume': actual_export_volume\n",
    "    }\n",
    "    \n",
    "    print(f'Event Date: {event_date}')\n",
    "    print(f'Average Abnormal Returns (AAR): {AAR}')\n",
    "    print(f'Cumulative Abnormal Returns (CAR): {CAR.iloc[-1]}')\n",
    " \n",
    "\n",
    "# 比较两次关税调整的影响差异\n",
    "event1_date = tariff_change_dates[0]\n",
    "event2_date = tariff_change_dates[1]\n",
    "\n",
    "event1_normal_volume = results[event1_date]['normal_export_volume']\n",
    "event1_CAR = results[event1_date]['CAR']\n",
    "event1_actual_volume = results[event1_date]['actual_export_volume']\n",
    "\n",
    "event2_normal_volume = results[event2_date]['normal_export_volume']\n",
    "event2_CAR = results[event2_date]['CAR']\n",
    "event2_actual_volume = results[event2_date]['actual_export_volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Study 的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# 设置图形大小\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# 定义颜色\n",
    "color_event2_car = '#FF9999'\n",
    "color_event1_car = '#66B3FF'\n",
    "color_normal_volume = '#009E73'\n",
    "color_actual_volume = '#D55E00'\n",
    "# 绘制CAR和正常出口量\n",
    "line1, = plt.plot(event1_CAR.index, event1_CAR, label=f'CAR around {event1_date.date()}', linestyle='--', color=color_event1_car, linewidth=2)\n",
    "line2, = plt.plot(event2_CAR.index, event2_CAR, label=f'CAR around {event2_date.date()}', linestyle='--', color=color_event2_car, linewidth=2)\n",
    "line3, = plt.plot(event1_normal_volume.index, event1_normal_volume, label=f'Normal Export Volume around {event1_date.date()}', linestyle='-.', color=color_normal_volume, linewidth=2)\n",
    "line4, = plt.plot(event2_normal_volume.index, event2_normal_volume, label=f'Normal Export Volume around {event2_date.date()}', linestyle='-.', color=color_normal_volume, linewidth=2)\n",
    "line5, = plt.plot(event1_actual_volume.index, event1_actual_volume, label=f'Actual Export Volume around {event1_date.date()}', linestyle='-', color=color_actual_volume, linewidth=2)\n",
    "line6, = plt.plot(event2_actual_volume.index, event2_actual_volume, label=f'Actual Export Volume around {event2_date.date()}', linestyle='-', color=color_actual_volume, linewidth=2)\n",
    "# 增加阴影效果\n",
    "plt.fill_between(event1_CAR.index, event1_CAR, color=color_event1_car, alpha=0.3)\n",
    "plt.fill_between(event2_CAR.index, event2_CAR, color=color_event2_car, alpha=0.3)\n",
    "\n",
    "# 绘制关税变化事件的垂直线\n",
    "plt.axvline(event1_date, color='b', linestyle=':', linewidth=2, label='First Tariff Change')\n",
    "plt.axvline(event2_date, color='r', linestyle=':', linewidth=2, label='Second Tariff Change')\n",
    "\n",
    "# # 设置X轴标签和Y轴标签\n",
    "# plt.xlabel('Date', fontsize=14)\n",
    "# plt.ylabel('Pet Food to US/Kg', fontsize=14)\n",
    "\n",
    "# # 设置标题\n",
    "# plt.title('Comparison of Cumulative Abnormal Returns (CAR) with Actual Export Volume', fontsize=16)\n",
    "\n",
    "# # 调整日期格式\n",
    "# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "# plt.xticks(rotation=45)\n",
    "# 设置X轴标签和Y轴标签，并使用LaTeX语法加粗\n",
    "plt.xlabel(r'$\\bf{Date}$', fontsize=14)\n",
    "plt.ylabel(r'$\\bf{Pet\\ Food\\ to\\ US/Kg}$', fontsize=14)\n",
    "\n",
    "# 设置标题，并使用LaTeX语法加粗\n",
    "plt.title(r'$\\bf{Comparison\\ of\\ Cumulative\\ Abnormal\\ Returns\\ (CAR)\\ with\\ Actual\\ Export\\ Volume}$', fontsize=16)\n",
    "\n",
    "# 添加网格\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 绘制第一次加征关税的图例（左上角）\n",
    "first_tariff_change_legend = plt.legend(handles=[line1, line3, line5],\n",
    "                                        loc='lower left',\n",
    "                                        bbox_to_anchor=(0.05 ,0.05),\n",
    "                                        ncol=1,\n",
    "                                        title='First Tariff Change',\n",
    "                                        frameon=True,\n",
    "                                        shadow=True,\n",
    "                                        fancybox=True)\n",
    "\n",
    "# 绘制第二次加征关税的图例（右下角）\n",
    "second_tariff_change_legend = plt.legend(handles=[line2, line4, line6],\n",
    "                                         loc='lower right',\n",
    "                                         bbox_to_anchor=(0.95, 0.05),\n",
    "                                         ncol=1,\n",
    "                                         title='Second Tariff Change',\n",
    "                                         frameon=True,\n",
    "                                         shadow=True,\n",
    "                                         fancybox=True)\n",
    "\n",
    "# 显示图例\n",
    "plt.gca().add_artist(first_tariff_change_legend)\n",
    "plt.gca().add_artist(second_tariff_change_legend)\n",
    "\n",
    "# 自动调整布局\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDICE  \n",
    "**好看的画图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 9))\n",
    "col = 'Cat dog food consumption'\n",
    "plt.plot(years, df[col], 'o', label='Original Data', linewidth=2)\n",
    "plt.plot(months, interpolated_data_cubic[col], '-', label='Cubic Spline Interpolation', linewidth=2)\n",
    "plt.plot(months, interpolated_data_linear[col], '--', label='Linear Interpolation', linewidth=2)\n",
    "plt.plot(months, interpolated_data_akima[col], ':', label='Akima Interpolation', linewidth=2)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.title(r'\\textbf{Cat dog food consumption}', fontsize=16)\n",
    "plt.xlabel(r'\\textbf{Year}', fontsize=14)\n",
    "plt.ylabel(r'\\textbf{Number/1e8}', fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
